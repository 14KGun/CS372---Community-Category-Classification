{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import needed library and declare list of category column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "crawledDataSetNames = ['dataset-reddit-business.csv', 'dataset-reddit-entertainment.csv', \n",
    "'dataset-reddit-parenting.csv', 'dataset-reddit-politics.csv', \n",
    "'dataset-reddit-sports.csv', 'dataset-reddit-travel.csv']\n",
    "\n",
    "categories = ['business', 'entertainment', 'parenting', 'politics', 'sports', 'travel']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(originData):\n",
    "    data = originData.copy()\n",
    "    #make all characters to lower case\n",
    "    data['text']=data['text'].apply(lambda x: str.lower(x))\n",
    "\n",
    "    #word tokenization\n",
    "    data['text'] = data['text'].apply(lambda x: wordpunct_tokenize(x))\n",
    "\n",
    "    #delete non alphabets word\n",
    "    data['text'] = data['text'].apply(lambda x: list(word for word in x if word.isalpha()))\n",
    "\n",
    "    print('preprocessing finished. 1 / 6')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "declare making word dictionary function and making Bag Of words Column function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makingWordDict(originData):\n",
    "    data = originData.copy()\n",
    "    listedText=data['text'].tolist()\n",
    "    allTextList = []\n",
    "    for l in listedText:\n",
    "        allTextList.extend(l)\n",
    "    numOfdict = len(set(allTextList))\n",
    "    \n",
    "    numOfFeatureWords = int(numOfdict*1)\n",
    "\n",
    "    fdist = FreqDist(allTextList)\n",
    "\n",
    "    wordDict = list(word for word, freq in fdist.most_common(numOfFeatureWords))\n",
    "\n",
    "    print('making word dictionary finished. 2 / 6')\n",
    "    return wordDict\n",
    "\n",
    "def makingBagOfWordsCol(originData, wordDict):\n",
    "    data = originData.copy()\n",
    "    def bagOfWords(tokens):\n",
    "        d = defaultdict(int,{ word:0 for word in wordDict })\n",
    "\n",
    "        for token in tokens:\n",
    "            d[token]+=1\n",
    "\n",
    "        return list(d.values())[:len(wordDict)]\n",
    "\n",
    "    data['bagOfWords'] = data['text'].apply(lambda x: bagOfWords(x))\n",
    "\n",
    "    print('making bag of words column finished. 3 / 6')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "declare divide trainset and testset function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divideTrainAndTest(originData):\n",
    "    data = originData.copy()\n",
    "    dataSize = data.shape[0]\n",
    "\n",
    "    trainSplit = 0.7\n",
    "    testSplit = 0.3\n",
    "\n",
    "    trainSize = int(dataSize * trainSplit)\n",
    "    testSize = int(dataSize * testSplit)\n",
    "\n",
    "    print('trainSize: ', trainSize)\n",
    "    print('testSize: ', testSize)\n",
    "\n",
    "    trainSet = data.iloc[:trainSize, :]\n",
    "    testSet = data.iloc[trainSize:, :]\n",
    "\n",
    "    print('dividing train and test set finished. 4 / 6')\n",
    "    return trainSet, testSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "declare build a Bag of Words model and fit by using train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildAndFitModel(trainSet):\n",
    "    classifier = MultinomialNB()\n",
    "    trainX = np.array(trainSet['bagOfWords'].tolist())\n",
    "    trainY = np.array(trainSet['category'].tolist())\n",
    "    \n",
    "    classifier.fit(trainX, trainY)\n",
    "\n",
    "    print('building and fitting the model finished. 5 / 6')\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "declare evaluate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(classifier, testSet):\n",
    "    testX = np.array(testSet['bagOfWords'].tolist())\n",
    "    testY = np.array(testSet['category'].tolist())\n",
    "    \n",
    "    predY = classifier.predict(testX)\n",
    "    predYProb = classifier.predict_proba(testX)\n",
    "    accuracy = accuracy_score(testY, predY)\n",
    "    f1Score = \n",
    "\n",
    "    print('evaluating the model finished. 6 / 6')\n",
    "    return accuracy, predYProb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. read the crawled Dataset of reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text       category\n",
      "0     Chinese Embassy voices 'grave concern' over ac...         sports\n",
      "1     I don't regret being a parent My 2 year old so...      parenting\n",
      "2     Miss USA 2019 Cheslie Kryst Dead at 30 from Su...  entertainment\n",
      "3                 Please Stop Trying To Sell Me Crypto   entertainment\n",
      "4     My son thinks he can swim but he cannot Hi the...      parenting\n",
      "...                                                 ...            ...\n",
      "5936  [TooGooden] Players currently in Covid protoco...         sports\n",
      "5937                  My 2020 trip to the Amalfi Coast          travel\n",
      "5938  Sean Penn In Ukraine: Putin Has Made a 'Horrib...  entertainment\n",
      "5939  McCarthy threatens to strip GOP members of com...       politics\n",
      "5940          My first time in magical Petra (Jordan).          travel\n",
      "\n",
      "[5941 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "rawRedditDataset = pd.DataFrame(columns = {'text','category'})\n",
    "for crawledDataSetName in crawledDataSetNames:\n",
    "    #read csv files\n",
    "    df = pd.read_csv('./dataset/%s'%crawledDataSetName)\n",
    "    \n",
    "    #replace Nan to empty string\n",
    "    df = df.fillna('')\n",
    "\n",
    "    #'titleAndContent' Column is concat of 'title' and 'content'\n",
    "    df['text']=df['title']+' '+df['content']\n",
    "    rawRedditDataset = pd.concat([rawRedditDataset, df[['text', 'category']]])\n",
    "\n",
    "rawRedditDataset=rawRedditDataset[['text', 'category']]\n",
    "#shuffle row\n",
    "rawRedditDataset=rawRedditDataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(rawRedditDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. read the news category dataset(from kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text       category\n",
      "0      Luke Bryan And Karen Fairchild Get The Crowd R...  entertainment\n",
      "1      Miley Cyrus Isn't Sorry About That Controversi...  entertainment\n",
      "2      Ed Oâ€™Neill Once Mistook Britney Spears For A B...  entertainment\n",
      "3      There's No Silver Bullet For Solving School Lu...       politics\n",
      "4      Dirty I cannot brush my hair today/There is no...      parenting\n",
      "...                                                  ...            ...\n",
      "11995  'How Many Reds Does It Take To Catch A Pop Fly...         sports\n",
      "11996  Obama Wants His Alaska Trip To Be The 'Punctua...       politics\n",
      "11997  Entrepreneur Leadership Today Demands a Human ...       business\n",
      "11998  Don't Say This: The Cruelest, Most Ignorant Pa...      parenting\n",
      "11999  Marissa Mayer Calls Out Media For Sexist Cover...       business\n",
      "\n",
      "[12000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "originNewsCategory = ['BUSINESS', 'ENTERTAINMENT', 'PARENTING', 'POLITICS', 'SPORTS', 'TRAVEL']\n",
    "\n",
    "rawNewsDataset = pd.read_json(\"./dataset/News_category_Dataset_v2.json\", lines=True)\n",
    "rawNewsDataset = rawNewsDataset[rawNewsDataset['category'].isin(originNewsCategory)]\n",
    "rawNewsDataset = rawNewsDataset.groupby('category').sample(n=2000)\n",
    "rawNewsDataset['category'] = rawNewsDataset['category'].apply(lambda x: x.lower())\n",
    "\n",
    "rawNewsDataset['text'] = rawNewsDataset['headline'] + ' ' + rawNewsDataset['short_description']\n",
    "rawNewsDataset = rawNewsDataset[['text','category']]\n",
    "\n",
    "#shuffle row\n",
    "rawNewsDataset=rawNewsDataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(rawNewsDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. the model only using reddit crawled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing finished. 1 / 6\n",
      "making word dictionary finished. 2 / 6\n",
      "making bag of words column finished. 3 / 6\n",
      "trainSize:  4158\n",
      "testSize:  1782\n",
      "dividing train and test set finished. 4 / 6\n",
      "building and fitting the model finished. 5 / 6\n",
      "evaluating the model finished. 6 / 6\n",
      "only reddit dataset model accuracy:  0.8457655636567583\n"
     ]
    }
   ],
   "source": [
    "redditDataset = preprocessing(rawRedditDataset)\n",
    "redditWordDict = makingWordDict(redditDataset)\n",
    "BOWredditDataset = makingBagOfWordsCol(redditDataset, redditWordDict)\n",
    "\n",
    "redditTrainSet, redditTestSet = divideTrainAndTest(BOWredditDataset)\n",
    "\n",
    "redditClassifier = buildAndFitModel(redditTrainSet)\n",
    "\n",
    "redditAccuracy, redditPredYProb = evaluate(redditClassifier, redditTestSet)\n",
    "\n",
    "print('only reddit dataset model accuracy: ', redditAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. the model only using news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing finished. 1 / 6\n",
      "making word dictionary finished. 2 / 6\n",
      "making bag of words column finished. 3 / 6\n",
      "trainSize:  8400\n",
      "testSize:  3600\n",
      "dividing train and test set finished. 4 / 6\n",
      "building and fitting the model finished. 5 / 6\n",
      "evaluating the model finished. 6 / 6\n",
      "only news dataset model accuracy:  0.7830555555555555\n"
     ]
    }
   ],
   "source": [
    "newsDataset = preprocessing(rawNewsDataset)\n",
    "newsWordDict = makingWordDict(newsDataset)\n",
    "BOWnewsDataset = makingBagOfWordsCol(newsDataset, newsWordDict)\n",
    "\n",
    "newsTrainSet, newsTestSet = divideTrainAndTest(BOWnewsDataset)\n",
    "\n",
    "newsClassifier = buildAndFitModel(newsTrainSet)\n",
    "\n",
    "newsAccuracy, newsPredYProb = evaluate(newsClassifier, newsTestSet)\n",
    "\n",
    "print('only news dataset model accuracy: ', newsAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. the model using both reddit and news datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing finished. 1 / 6\n",
      "making word dictionary finished. 2 / 6\n",
      "making bag of words column finished. 3 / 6\n",
      "trainSize:  12558\n",
      "testSize:  5382\n",
      "dividing train and test set finished. 4 / 6\n",
      "building and fitting the model finished. 5 / 6\n",
      "evaluating the model finished. 6 / 6\n",
      "using both reddit and both dataset model accuracy:  0.7505108675459781\n"
     ]
    }
   ],
   "source": [
    "rawBothDataset = pd.concat([rawRedditDataset, rawNewsDataset])\n",
    "\n",
    "bothDataset = preprocessing(rawBothDataset)\n",
    "bothWordDict = makingWordDict(bothDataset)\n",
    "BOWbothDataset = makingBagOfWordsCol(bothDataset, bothWordDict)\n",
    "\n",
    "bothTrainSet, bothTestSet = divideTrainAndTest(BOWbothDataset)\n",
    "\n",
    "bothClassifier = buildAndFitModel(bothTrainSet)\n",
    "\n",
    "bothAccuracy, bothPredYProb = evaluate(bothClassifier, bothTestSet)\n",
    "\n",
    "print('using both reddit and both dataset model accuracy: ', bothAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. predict category of reddit testset by the model only trained by news train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4h/gkd9rvld5sd12215ppr2x1pc0000gn/T/ipykernel_7770/1202364807.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['bagOfWords'] = data['text'].apply(lambda x: bagOfWords(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making bag of words column finished. 3 / 6\n",
      "evaluating the model finished. 6 / 6\n",
      "predicting the category of reddit test set on news-trained model accuracy:  0.802019068984857\n"
     ]
    }
   ],
   "source": [
    "BOWredditTestsetUsingNewsWordDict = makingBagOfWordsCol(redditTestSet, newsWordDict)\n",
    "\n",
    "redditOnNewsAccuracy, redditOnNewsPredYProb = evaluate(newsClassifier, BOWredditTestsetUsingNewsWordDict)\n",
    "\n",
    "print('predicting the category of reddit test set on news-trained model accuracy: ', redditOnNewsAccuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. predict category of news testset by the model only trained by reddit train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4h/gkd9rvld5sd12215ppr2x1pc0000gn/T/ipykernel_7770/1202364807.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['bagOfWords'] = data['text'].apply(lambda x: bagOfWords(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making bag of words column finished. 3 / 6\n",
      "evaluating the model finished. 6 / 6\n",
      "predicting the category of reddit test set on news-trained model accuracy:  0.5166666666666667\n"
     ]
    }
   ],
   "source": [
    "BOWnewsTestsetUsingRedditWordDict = makingBagOfWordsCol(newsTestSet, redditWordDict)\n",
    "\n",
    "newsOnRedditAccuracy, newsOnRedditPredYProb = evaluate(redditClassifier, BOWnewsTestsetUsingRedditWordDict)\n",
    "\n",
    "print('predicting the category of reddit test set on news-trained model accuracy: ', newsOnRedditAccuracy)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "275f008190911cb19b5a5a06a55a9f7dab82df90026b87cc1b47af86a83cd370"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
