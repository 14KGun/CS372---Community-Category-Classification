{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "crawledDataSetNames = ['dataset-reddit-business.csv', 'dataset-reddit-entertainment.csv', \n",
    "'dataset-reddit-parenting.csv', 'dataset-reddit-politics.csv', \n",
    "'dataset-reddit-sports.csv', 'dataset-reddit-travel.csv']\n",
    "\n",
    "categories = ['business', 'entertainment', 'parenting', 'politics', 'sports', 'travel']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read csv and concat title and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text       category\n",
      "0     Bills receiver Cole Beasley has been fined mul...         sports\n",
      "1     Kaishu Hirano goes sky high during the Menâ€™s S...         sports\n",
      "2     Will Smith Did a Bad, Bad Thing. Slapping Chri...  entertainment\n",
      "3     Russian Stocks Tumble Most Since Crimea Annexa...       business\n",
      "4     Trump Funding Network Paid $4.3 Million To Peo...       politics\n",
      "...                                                 ...            ...\n",
      "5936                           Solo Trip to Lebanon ðŸ‡±ðŸ‡§          travel\n",
      "5937  'Fire DeJoy' Demand Intensifies as 10-Year Pla...       politics\n",
      "5938  California grocery workers vote to authorize s...       business\n",
      "5939  Historic Leak of Swiss Banking Records Reveals...       business\n",
      "5940  Jonathan Groff Calls Keanu Reeves the 'Greates...  entertainment\n",
      "\n",
      "[5941 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.DataFrame(columns = {'text','category'})\n",
    "for crawledDataSetName in crawledDataSetNames:\n",
    "    df = pd.read_csv('./dataset/%s'%crawledDataSetName)\n",
    "    \n",
    "    #replace Nan to empty string\n",
    "    df = df.fillna('')\n",
    "\n",
    "    #'titleAndContent' Column is concat of 'title' and 'content'\n",
    "    df['text']=df['title']+' '+df['content']\n",
    "    dataset = pd.concat([dataset, df[['text', 'category']]])\n",
    "\n",
    "#shuffle row\n",
    "dataset=dataset.sample(frac=1).reset_index(drop=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text       category\n",
      "0     [bills, receiver, cole, beasley, has, been, fi...         sports\n",
      "1     [kaishu, hirano, goes, sky, high, during, the,...         sports\n",
      "2     [will, smith, did, a, bad, bad, thing, slappin...  entertainment\n",
      "3     [russian, stocks, tumble, most, since, crimea,...       business\n",
      "4     [trump, funding, network, paid, million, to, p...       politics\n",
      "...                                                 ...            ...\n",
      "5936                          [solo, trip, to, lebanon]         travel\n",
      "5937  [fire, dejoy, demand, intensifies, as, year, p...       politics\n",
      "5938  [california, grocery, workers, vote, to, autho...       business\n",
      "5939  [historic, leak, of, swiss, banking, records, ...       business\n",
      "5940  [jonathan, groff, calls, keanu, reeves, the, g...  entertainment\n",
      "\n",
      "[5941 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "#make all characters to lower case\n",
    "dataset['text']=dataset['text'].apply(lambda x: str.lower(x))\n",
    "\n",
    "#word tokenization\n",
    "dataset['text'] = dataset['text'].apply(lambda x: wordpunct_tokenize(x))\n",
    "\n",
    "#delete stopwords and punctuation\n",
    "stopwordList = set(stopwords.words('english') + list(string.punctuation))\n",
    "dataset['text'] = dataset['text'].apply(lambda x: list(word for word in x if word.isalpha()))\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make bag of words column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of word dict:  17677\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "allTextList = sum(dataset['text'].tolist(),[])\n",
    "numOfdict = len(set(allTextList))\n",
    "print('the number of word dict: ',numOfdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [0, 1, 3, 1, 2, 0, 1, 3, 0, 0, 2, 0, 0, 0, 0, ...\n",
      "1       [0, 0, 1, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
      "2       [0, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "3       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "4       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "                              ...                        \n",
      "5936    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "5937    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "5938    [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "5939    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "5940    [0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, ...\n",
      "Name: bagOfWords, Length: 5941, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#the number of words to use for training\n",
    "numOfFeatureWords = numOfdict\n",
    "\n",
    "\n",
    "fdist = FreqDist(allTextList)\n",
    "wordDict = list(word for word, freq in fdist.most_common(numOfFeatureWords))\n",
    "def bagOfWords(tokens):\n",
    "    d = defaultdict(int,{ word:0 for word in wordDict })\n",
    "    for token in tokens:\n",
    "        d[token]+=1\n",
    "    ret = []\n",
    "    for key, val in d.items():\n",
    "        ret.append(val)\n",
    "    return ret[:numOfFeatureWords]\n",
    "\n",
    "dataset['bagOfWords'] = dataset['text'].apply(lambda x: bagOfWords(x))\n",
    "\n",
    "print(dataset['bagOfWords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "divide trainset and testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainSize:  4752\n",
      "testSize:  1188\n",
      "4752\n",
      "1189\n"
     ]
    }
   ],
   "source": [
    "trainSplit = 0.8\n",
    "testSplit = 0.2\n",
    "\n",
    "datasetSize = dataset.shape[0]\n",
    "\n",
    "trainSize = int(datasetSize * trainSplit)\n",
    "testSize = int(datasetSize * testSplit)\n",
    "\n",
    "print('trainSize: ', trainSize)\n",
    "print('testSize: ', testSize)\n",
    "\n",
    "trainSet = dataset.iloc[:trainSize, :]\n",
    "testSet = dataset.iloc[trainSize:, :]\n",
    "\n",
    "print(trainSet.shape[0])\n",
    "print(testSet.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4752, 17677) (4752,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "classifier = GaussianNB()\n",
    "trainX = np.array(trainSet['bagOfWords'].tolist())\n",
    "trainY = np.array(trainSet['category'].tolist())\n",
    "print(trainX.shape, trainY.shape)\n",
    "classifier.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1189, 17677) (1189,)\n",
      "accuracy : 0.7981497056349874\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "testX = np.array(testSet['bagOfWords'].tolist())\n",
    "testY = np.array(testSet['category'].tolist())\n",
    "print(testX.shape, testY.shape)\n",
    "predY = classifier.predict(testX)\n",
    "accuracy = accuracy_score(testY, predY)\n",
    "print('accuracy :', accuracy)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9920a9191bbd0883129cd4e7291d7025cf6acd0cb1f54d8654d4c35c2558a4dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
