{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import needed library and declare list of category column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "crawledDataSetNames = ['dataset-reddit-business.csv', 'dataset-reddit-entertainment.csv', \n",
    "'dataset-reddit-parenting.csv', 'dataset-reddit-politics.csv', \n",
    "'dataset-reddit-sports.csv', 'dataset-reddit-travel.csv']\n",
    "\n",
    "categories = ['business', 'entertainment', 'parenting', 'politics', 'sports', 'travel']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(originData):\n",
    "    data = originData.copy()\n",
    "    #make all characters to lower case\n",
    "    data['text']=data['text'].apply(lambda x: str.lower(x))\n",
    "\n",
    "    #word tokenization\n",
    "    data['text'] = data['text'].apply(lambda x: wordpunct_tokenize(x))\n",
    "\n",
    "    #delete non alphabets word\n",
    "    data['text'] = data['text'].apply(lambda x: list(word for word in x if word.isalpha()))\n",
    "\n",
    "    print('preprocessing finished. 1 / 6')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "declare making word dictionary function and making Bag Of words Column function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makingWordDict(data):\n",
    "    listedText=data['text'].tolist()\n",
    "    allTextList = []\n",
    "    for l in listedText:\n",
    "        allTextList.extend(l)\n",
    "    numOfdict = len(set(allTextList))\n",
    "    \n",
    "    numOfFeatureWords = int(numOfdict*0.95)\n",
    "\n",
    "    fdist = FreqDist(allTextList)\n",
    "\n",
    "    wordDict = list(word for word, freq in fdist.most_common(numOfFeatureWords))\n",
    "\n",
    "    print('making word dictionary finished. 2 / 6')\n",
    "    return wordDict\n",
    "\n",
    "def makingBagOfWordsCol(data, wordDict):\n",
    "\n",
    "    def bagOfWords(tokens):\n",
    "        d = defaultdict(int,{ word:0 for word in wordDict })\n",
    "\n",
    "        for token in tokens:\n",
    "            d[token]+=1\n",
    "\n",
    "        return list(d.values())[:len(wordDict)]\n",
    "\n",
    "    data['bagOfWords'] = data['text'].apply(lambda x: bagOfWords(x))\n",
    "\n",
    "    print('making bag of words column finished. 3 / 6')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "declare divide trainset and testset function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divideTrainAndTest(data):\n",
    "    dataSize = data.shape[0]\n",
    "\n",
    "    trainSplit = 0.7\n",
    "    testSplit = 0.3\n",
    "\n",
    "    trainSize = int(dataSize * trainSplit)\n",
    "    testSize = int(dataSize * testSplit)\n",
    "\n",
    "    print('trainSize: ', trainSize)\n",
    "    print('testSize: ', testSize)\n",
    "\n",
    "    trainSet = data.iloc[:trainSize, :]\n",
    "    testSet = data.iloc[trainSize:, :]\n",
    "\n",
    "    print('dividing train and test set finished. 4 / 6')\n",
    "    return trainSet, testSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "declare build a Bag of Words model and fit by using train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildAndFitModel(trainSet):\n",
    "    classifier = MultinomialNB()\n",
    "    trainX = np.array(trainSet['bagOfWords'].tolist())\n",
    "    trainY = np.array(trainSet['category'].tolist())\n",
    "    \n",
    "    classifier.fit(trainX, trainY)\n",
    "\n",
    "    print('building and fitting the model finished. 5 / 6')\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "declare evaluate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(classifier, testSet):\n",
    "    testX = np.array(testSet['bagOfWords'].tolist())\n",
    "    testY = np.array(testSet['category'].tolist())\n",
    "    \n",
    "    predY = classifier.predict(testX)\n",
    "    predYProb = classifier.predict_proba(testX)\n",
    "    accuracy = accuracy_score(testY, predY)\n",
    "\n",
    "    print('evaluating the model finished. 6 / 6')\n",
    "    return accuracy, predYProb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. read the crawled Dataset of reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text       category\n",
      "0     Former Jets, Rams RB Zac Stacy has been arrest...         sports\n",
      "1     Should i travel the world alone as a woman if ...         travel\n",
      "2     Netflix eyes foray into video games: the strea...       business\n",
      "3     Grimes wanted to direct ‘Dune,’ says she was f...  entertainment\n",
      "4     Jon Voight, Outspoken GOP Supporter, Calls For...  entertainment\n",
      "...                                                 ...            ...\n",
      "5936  Alex Jones sat with the Jan. 6 panel and repea...       politics\n",
      "5937    Armenia, little country full of big monuments.          travel\n",
      "5938  Amber Heard Rolls Her Eyes After Johnny Depp's...  entertainment\n",
      "5939  Not Yet Known Whether Charges Will Be Filed In...  entertainment\n",
      "5940  My parents would rather not meet their first g...      parenting\n",
      "\n",
      "[5941 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "rawRedditDataset = pd.DataFrame(columns = {'text','category'})\n",
    "for crawledDataSetName in crawledDataSetNames:\n",
    "    #read csv files\n",
    "    df = pd.read_csv('./dataset/%s'%crawledDataSetName)\n",
    "    \n",
    "    #replace Nan to empty string\n",
    "    df = df.fillna('')\n",
    "\n",
    "    #'titleAndContent' Column is concat of 'title' and 'content'\n",
    "    df['text']=df['title']+' '+df['content']\n",
    "    rawRedditDataset = pd.concat([rawRedditDataset, df[['text', 'category']]])\n",
    "\n",
    "rawRedditDataset=rawRedditDataset[['text', 'category']]\n",
    "#shuffle row\n",
    "rawRedditDataset=rawRedditDataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(rawRedditDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. read the news category dataset(from kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text       category\n",
      "0      Darren Sharper May Have Penis Monitored As Par...         sports\n",
      "1      Slavic Cool? “Slavic cool” is everywhere these...         travel\n",
      "2      FIFA Whistleblower Chuck Blazer Dead At 72 He ...         sports\n",
      "3      5 Things Not to Say to a Pregnant Woman Perhap...      parenting\n",
      "4      How The Volatile Setting Of Netflix's 'Ozark' ...  entertainment\n",
      "...                                                  ...            ...\n",
      "11995  Robin Thicke's Acting Debut Will Make You Crin...  entertainment\n",
      "11996  Simone Biles Makes History With 4th Consecutiv...         sports\n",
      "11997  Spilled Milk: Photoshopping My Neck Once upon ...      parenting\n",
      "11998  Winnie The Pooh Is Trending On Twitter For The...         sports\n",
      "11999  Top GOP Operative: Mike Pence Once Thought Tru...       politics\n",
      "\n",
      "[12000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "originNewsCategory = ['BUSINESS', 'ENTERTAINMENT', 'PARENTING', 'POLITICS', 'SPORTS', 'TRAVEL']\n",
    "\n",
    "rawNewsDataset = pd.read_json(\"./dataset/News_category_Dataset_v2.json\", lines=True)\n",
    "rawNewsDataset = rawNewsDataset[rawNewsDataset['category'].isin(originNewsCategory)]\n",
    "rawNewsDataset = rawNewsDataset.groupby('category').sample(n=2000)\n",
    "rawNewsDataset['category'] = rawNewsDataset['category'].apply(lambda x: x.lower())\n",
    "\n",
    "rawNewsDataset['text'] = rawNewsDataset['headline'] + ' ' + rawNewsDataset['short_description']\n",
    "rawNewsDataset = rawNewsDataset[['text','category']]\n",
    "\n",
    "#shuffle row\n",
    "rawNewsDataset=rawNewsDataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print(rawNewsDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. the model only using reddit crawled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing finished. 1 / 6\n",
      "making word dictionary finished. 2 / 6\n",
      "making bag of words column finished. 3 / 6\n",
      "trainSize:  4158\n",
      "testSize:  1782\n",
      "dividing train and test set finished. 4 / 6\n",
      "building and fitting the model finished. 5 / 6\n",
      "evaluating the model finished. 6 / 6\n",
      "only reddit dataset model accuracy:  0.8424004486819966\n"
     ]
    }
   ],
   "source": [
    "redditDataset = preprocessing(rawRedditDataset)\n",
    "redditWordDict = makingWordDict(redditDataset)\n",
    "BOWredditDataset = makingBagOfWordsCol(redditDataset, redditWordDict)\n",
    "\n",
    "redditTrainSet, redditTestSet = divideTrainAndTest(BOWredditDataset)\n",
    "\n",
    "redditClassifier = buildAndFitModel(redditTrainSet)\n",
    "\n",
    "redditAccuracy, redditPredYProb = evaluate(redditClassifier, redditTestSet)\n",
    "\n",
    "print('only reddit dataset model accuracy: ', redditAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. the model only using news dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing finished. 1 / 6\n",
      "making word dictionary finished. 2 / 6\n",
      "making bag of words column finished. 3 / 6\n",
      "trainSize:  8400\n",
      "testSize:  3600\n",
      "dividing train and test set finished. 4 / 6\n",
      "building and fitting the model finished. 5 / 6\n",
      "evaluating the model finished. 6 / 6\n",
      "only news dataset model accuracy:  0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "newsDataset = preprocessing(rawNewsDataset)\n",
    "newsWordDict = makingWordDict(newsDataset)\n",
    "BOWnewsDataset = makingBagOfWordsCol(newsDataset, newsWordDict)\n",
    "\n",
    "newsTrainSet, newsTestSet = divideTrainAndTest(BOWnewsDataset)\n",
    "\n",
    "newsClassifier = buildAndFitModel(newsTrainSet)\n",
    "\n",
    "newsAccuracy, newsPredYProb = evaluate(newsClassifier, newsTestSet)\n",
    "\n",
    "print('only news dataset model accuracy: ', newsAccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. the model using both reddit and news datasets."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "275f008190911cb19b5a5a06a55a9f7dab82df90026b87cc1b47af86a83cd370"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
