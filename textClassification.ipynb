{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "crawledDataSetNames = ['dataset-reddit-business.csv', 'dataset-reddit-entertainment.csv', \n",
    "'dataset-reddit-parenting.csv', 'dataset-reddit-politics.csv', \n",
    "'dataset-reddit-sports.csv', 'dataset-reddit-travel.csv']\n",
    "\n",
    "categories = ['business', 'entertainment', 'parenting', 'politics', 'sports', 'travel']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read csv and concat title and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text       category\n",
      "0     Kyoto with no Tourist feels so strange but odd...         travel\n",
      "1     Israeli soccer fans subjected to antisemitic a...         sports\n",
      "2     Requiring Vaccination to See New Baby My husba...      parenting\n",
      "3     ABBA’s greatest hits album, “ABBA Gold: Greate...  entertainment\n",
      "4     Jason Momoa already wants a 'Villeneuve Cut' o...  entertainment\n",
      "...                                                 ...            ...\n",
      "5936  Another cool-down post - Photos from my four c...         travel\n",
      "5937  My transatlantic flight today was the emptiest...         travel\n",
      "5938                         Edinburgh, Scotland. 2022          travel\n",
      "5939  Cristiano Ronaldo scores in his first Champion...         sports\n",
      "5940  George Bush delights Democrats, infuriates MAG...       politics\n",
      "\n",
      "[5941 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.DataFrame(columns = {'text','category'})\n",
    "for crawledDataSetName in crawledDataSetNames:\n",
    "    df = pd.read_csv('./dataset/%s'%crawledDataSetName)\n",
    "    \n",
    "    #replace Nan to empty string\n",
    "    df = df.fillna('')\n",
    "\n",
    "    #'titleAndContent' Column is concat of 'title' and 'content'\n",
    "    df['text']=df['title']+' '+df['content']\n",
    "    dataset = pd.concat([dataset, df[['text', 'category']]])\n",
    "\n",
    "#shuffle row\n",
    "dataset=dataset.sample(frac=1).reset_index(drop=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text       category\n",
      "0     [kyoto, with, no, tourist, feels, so, strange,...         travel\n",
      "1     [israeli, soccer, fans, subjected, to, antisem...         sports\n",
      "2     [requiring, vaccination, to, see, new, baby, m...      parenting\n",
      "3     [abba, s, greatest, hits, album, abba, gold, g...  entertainment\n",
      "4     [jason, momoa, already, wants, a, villeneuve, ...  entertainment\n",
      "...                                                 ...            ...\n",
      "5936  [another, cool, down, post, photos, from, my, ...         travel\n",
      "5937  [my, transatlantic, flight, today, was, the, e...         travel\n",
      "5938                              [edinburgh, scotland]         travel\n",
      "5939  [cristiano, ronaldo, scores, in, his, first, c...         sports\n",
      "5940  [george, bush, delights, democrats, infuriates...       politics\n",
      "\n",
      "[5941 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "#make all characters to lower case\n",
    "dataset['text']=dataset['text'].apply(lambda x: str.lower(x))\n",
    "\n",
    "#word tokenization\n",
    "dataset['text'] = dataset['text'].apply(lambda x: wordpunct_tokenize(x))\n",
    "\n",
    "#delete stopwords and punctuation\n",
    "stopwordList = set(stopwords.words('english') + list(string.punctuation))\n",
    "dataset['text'] = dataset['text'].apply(lambda x: list(word for word in x if word.isalpha()))\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make bag of words column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of word dict:  17677\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "allTextList = sum(dataset['text'].tolist(),[])\n",
    "numOfdict = len(set(allTextList))\n",
    "print('the number of word dict: ',numOfdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1       [0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "2       [3, 8, 4, 5, 2, 5, 3, 1, 3, 0, 1, 3, 2, 2, 0, ...\n",
      "3       [0, 1, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
      "4       [2, 4, 1, 0, 2, 0, 2, 0, 1, 1, 0, 0, 0, 1, 0, ...\n",
      "                              ...                        \n",
      "5936    [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "5937    [1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "5938    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "5939    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "5940    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: bagOfWords, Length: 5941, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#the number of words to use for training\n",
    "numOfFeatureWords = int(numOfdict*0.95)\n",
    "\n",
    "fdist = FreqDist(allTextList)\n",
    "\n",
    "wordDict = list(word for word, freq in fdist.most_common(numOfFeatureWords))\n",
    "def bagOfWords(tokens):\n",
    "    d = defaultdict(int,{ word:0 for word in wordDict })\n",
    "    for token in tokens:\n",
    "        d[token]+=1\n",
    "    ret = []\n",
    "    for key, val in d.items():\n",
    "        ret.append(val)\n",
    "    return ret[:numOfFeatureWords]\n",
    "\n",
    "dataset['bagOfWords'] = dataset['text'].apply(lambda x: bagOfWords(x))\n",
    "\n",
    "print(dataset['bagOfWords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "divide trainset and testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainSize:  4158\n",
      "testSize:  1782\n",
      "4158\n",
      "1783\n"
     ]
    }
   ],
   "source": [
    "trainSplit = 0.7\n",
    "testSplit = 0.3\n",
    "\n",
    "datasetSize = dataset.shape[0]\n",
    "\n",
    "trainSize = int(datasetSize * trainSplit)\n",
    "testSize = int(datasetSize * testSplit)\n",
    "\n",
    "print('trainSize: ', trainSize)\n",
    "print('testSize: ', testSize)\n",
    "\n",
    "trainSet = dataset.iloc[:trainSize, :]\n",
    "testSet = dataset.iloc[trainSize:, :]\n",
    "\n",
    "print(trainSet.shape[0])\n",
    "print(testSet.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4158, 16793) (4158,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "classifier = MultinomialNB()\n",
    "trainX = np.array(trainSet['bagOfWords'].tolist())\n",
    "trainY = np.array(trainSet['category'].tolist())\n",
    "print(trainX.shape, trainY.shape)\n",
    "classifier.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1783, 16793) (1783,)\n",
      "accuracy : 0.8485698261357263\n",
      "[[1.82887945e-01 4.47672959e-02 1.19445014e-04 1.04053667e-01\n",
      "  8.88350292e-02 5.79336617e-01]\n",
      " [2.04454400e-18 1.69851805e-16 6.88674533e-06 3.32998752e-14\n",
      "  4.87969458e-15 9.99993113e-01]\n",
      " [5.53408307e-05 1.11941058e-04 1.84667887e-06 1.79680225e-05\n",
      "  9.99768537e-01 4.43662885e-05]\n",
      " ...\n",
      " [1.27074265e-01 5.76118036e-02 1.16244992e-03 5.41898684e-02\n",
      "  2.28557406e-01 5.31404206e-01]\n",
      " [1.29070918e-09 1.37034487e-09 4.65720951e-14 2.26428064e-11\n",
      "  9.99999997e-01 1.50221736e-10]\n",
      " [2.21910238e-04 3.20877405e-04 1.71759406e-09 9.98559876e-01\n",
      "  8.18865347e-04 7.84692956e-05]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "testX = np.array(testSet['bagOfWords'].tolist())\n",
    "testY = np.array(testSet['category'].tolist())\n",
    "print(testX.shape, testY.shape)\n",
    "predY = classifier.predict(testX)\n",
    "predYProb = classifier.predict_proba(testX)\n",
    "accuracy = accuracy_score(testY, predY)\n",
    "print('accuracy :', accuracy)\n",
    "print(predYProb)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9920a9191bbd0883129cd4e7291d7025cf6acd0cb1f54d8654d4c35c2558a4dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
