{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "crawledDataSetNames = ['dataset-reddit-business.csv', 'dataset-reddit-entertainment.csv', \n",
    "'dataset-reddit-parenting.csv', 'dataset-reddit-politics.csv', \n",
    "'dataset-reddit-sports.csv', 'dataset-reddit-travel.csv']\n",
    "\n",
    "categories = ['business', 'entertainment', 'parenting', 'politics', 'sports', 'travel']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read csv and concat title and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text   category\n",
      "0       This week in Bidenomics: The rich will survive    business\n",
      "1     Today my son asked my wife to legally adopt hi...  parenting\n",
      "2     Fiji has won gold in Rugby Sevens - Men’s at T...     sports\n",
      "3     Trump Ordered Staff to 'Bust Some Heads' of Bl...   politics\n",
      "4                    Fell in love with Porto, Portugal      travel\n",
      "...                                                 ...        ...\n",
      "5936  Lawmakers Talk Next Steps For Marijuana Bankin...   business\n",
      "5937  Norwegian women's beach-handball team forced t...     sports\n",
      "5938  Noam Chomsky: ‘Republican Party has drifted of...   politics\n",
      "5939  How employers steal billions of dollars from w...   business\n",
      "5940  Don’t hold back because of kids.. take them on...     travel\n",
      "\n",
      "[5941 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.DataFrame(columns = {'text','category'})\n",
    "for crawledDataSetName in crawledDataSetNames:\n",
    "    df = pd.read_csv('./dataset/%s'%crawledDataSetName)\n",
    "    \n",
    "    #replace Nan to empty string\n",
    "    df = df.fillna('')\n",
    "\n",
    "    #'titleAndContent' Column is concat of 'title' and 'content'\n",
    "    df['text']=df['title']+' '+df['content']\n",
    "    dataset = pd.concat([dataset, df[['text', 'category']]])\n",
    "\n",
    "#shuffle row\n",
    "dataset=dataset.sample(frac=1).reset_index(drop=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text   category\n",
      "0     [this, week, in, bidenomics, the, rich, will, ...   business\n",
      "1     [today, my, son, asked, my, wife, to, legally,...  parenting\n",
      "2     [fiji, has, won, gold, in, rugby, sevens, men,...     sports\n",
      "3     [trump, ordered, staff, to, some, heads, of, b...   politics\n",
      "4               [fell, in, love, with, porto, portugal]     travel\n",
      "...                                                 ...        ...\n",
      "5936  [lawmakers, talk, next, steps, for, marijuana,...   business\n",
      "5937  [norwegian, women, team, forced, to, pay, fine...     sports\n",
      "5938  [noam, chomsky, republican, party, has, drifte...   politics\n",
      "5939  [how, employers, steal, billions, of, dollars,...   business\n",
      "5940  [don, t, hold, back, because, of, kids, take, ...     travel\n",
      "\n",
      "[5941 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "#make all characters to lower case\n",
    "dataset['text']=dataset['text'].apply(lambda x: str.lower(x))\n",
    "\n",
    "#word tokenization\n",
    "dataset['text'] = dataset['text'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "#delete stopwords and punctuation\n",
    "stopwordList = set(stopwords.words('english') + list(string.punctuation))\n",
    "dataset['text'] = dataset['text'].apply(lambda x: list(word for word in x if word.isalpha()))\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make bag of words column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of word dict:  17395\n",
      "0       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1       [5, 3, 0, 1, 0, 7, 2, 0, 3, 2, 1, 1, 3, 0, 1, ...\n",
      "2       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "3       [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "4       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "                              ...                        \n",
      "5936    [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
      "5937    [0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "5938    [0, 1, 1, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...\n",
      "5939    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "5940    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: bagOfWords, Length: 5941, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "allTextList = sum(dataset['text'].tolist(),[])\n",
    "numOfdict = len(set(allTextList))\n",
    "print('the number of word dict: ',numOfdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1       [5, 3, 0, 1, 0, 7, 2, 0, 3, 2, 1, 1, 3, 0, 1, ...\n",
      "2       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "3       [0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "4       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "                              ...                        \n",
      "5936    [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
      "5937    [0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "5938    [0, 1, 1, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...\n",
      "5939    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "5940    [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: bagOfWords, Length: 5941, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#the number of words to use for training\n",
    "numOfFeatureWords = numOfdict-4000\n",
    "\n",
    "\n",
    "fdist = FreqDist(allTextList)\n",
    "wordDict = list(word for word, freq in fdist.most_common(numOfFeatureWords))\n",
    "def bagOfWords(tokens):\n",
    "    d = defaultdict(int,{ word:0 for word in wordDict })\n",
    "    for token in tokens:\n",
    "        d[token]+=1\n",
    "    ret = []\n",
    "    for key, val in d.items():\n",
    "        ret.append(val)\n",
    "    return ret[:numOfFeatureWords]\n",
    "\n",
    "dataset['bagOfWords'] = dataset['text'].apply(lambda x: bagOfWords(x))\n",
    "\n",
    "print(dataset['bagOfWords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add column of one hot encoding for category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       {'business': True, 'entertainment': False, 'pa...\n",
      "1       {'business': False, 'entertainment': False, 'p...\n",
      "2       {'business': False, 'entertainment': False, 'p...\n",
      "3       {'business': False, 'entertainment': False, 'p...\n",
      "4       {'business': False, 'entertainment': False, 'p...\n",
      "                              ...                        \n",
      "5936    {'business': True, 'entertainment': False, 'pa...\n",
      "5937    {'business': False, 'entertainment': False, 'p...\n",
      "5938    {'business': False, 'entertainment': False, 'p...\n",
      "5939    {'business': True, 'entertainment': False, 'pa...\n",
      "5940    {'business': False, 'entertainment': False, 'p...\n",
      "Name: labels, Length: 5941, dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataset['labels'] = dataset['category'].apply(lambda x: dict(defaultdict(bool,{ category: category == x for category in categories })))\n",
    "print(dataset['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "divide trainset and testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainSize:  4158\n",
      "testSize:  1782\n",
      "4158\n",
      "1783\n"
     ]
    }
   ],
   "source": [
    "trainSplit = 0.7\n",
    "testSplit = 0.3\n",
    "\n",
    "datasetSize = dataset.shape[0]\n",
    "\n",
    "trainSize = int(datasetSize * trainSplit)\n",
    "testSize = int(datasetSize * testSplit)\n",
    "\n",
    "print('trainSize: ', trainSize)\n",
    "print('testSize: ', testSize)\n",
    "\n",
    "trainSet = dataset.iloc[:trainSize, :]\n",
    "testSet = dataset.iloc[trainSize:, :]\n",
    "\n",
    "print(trainSet.shape[0])\n",
    "print(testSet.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4158, 13395) (4158,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "classifier = GaussianNB()\n",
    "trainX = np.array(trainSet['bagOfWords'].tolist())\n",
    "trainY = np.array(trainSet['category'].tolist())\n",
    "print(trainX.shape, trainY.shape)\n",
    "classifier.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1783, 13395) (1783,)\n",
      "accuracy : 0.7885586090858104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "testX = np.array(testSet['bagOfWords'].tolist())\n",
    "testY = np.array(testSet['category'].tolist())\n",
    "print(testX.shape, testY.shape)\n",
    "predY = classifier.predict(testX)\n",
    "accuracy = accuracy_score(testY, predY)\n",
    "print('accuracy :', accuracy)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9920a9191bbd0883129cd4e7291d7025cf6acd0cb1f54d8654d4c35c2558a4dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
