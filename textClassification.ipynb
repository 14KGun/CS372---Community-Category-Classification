{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "crawledDataSetNames = ['dataset-reddit-business.csv', 'dataset-reddit-entertainment.csv', \n",
    "'dataset-reddit-parenting.csv', 'dataset-reddit-politics.csv', \n",
    "'dataset-reddit-sports.csv', 'dataset-reddit-travel.csv']\n",
    "\n",
    "categories = ['business', 'entertainment', 'parenting', 'politics', 'sports', 'travel']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read csv and concat title and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       category                                               text\n",
      "0      politics  It’s clear capitalism isn’t working when US po...\n",
      "1      business  Nestle to suspend many products in Russia incl...\n",
      "2      politics  Capitol rioters called Nancy Pelosi's office l...\n",
      "3      business  Johnson & Johnson Plans to Split Into Two Publ...\n",
      "4        travel                                 My trip to Egypt! \n",
      "...         ...                                                ...\n",
      "5936   business  Peloton insiders sold nearly $500 million in s...\n",
      "5937     travel                      Cotswolds! Quite picturesque \n",
      "5938     travel  Just came back from a trip to Amsterdam and it...\n",
      "5939  parenting  is it ok to check the baby monitor at night? m...\n",
      "5940   business  Self-driving Waymo trucks to haul loads betwee...\n",
      "\n",
      "[5941 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.DataFrame(columns = {'text','category'})\n",
    "for crawledDataSetName in crawledDataSetNames:\n",
    "    df = pd.read_csv('./dataset/%s'%crawledDataSetName)\n",
    "    \n",
    "    #replace Nan to empty string\n",
    "    df = df.fillna('')\n",
    "\n",
    "    #'titleAndContent' Column is concat of 'title' and 'content'\n",
    "    df['text']=df['title']+' '+df['content']\n",
    "    dataset = pd.concat([dataset, df[['text', 'category']]])\n",
    "\n",
    "#shuffle row\n",
    "dataset=dataset.sample(frac=1).reset_index(drop=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       category                                               text\n",
      "0      politics  [it, s, clear, capitalism, isn, t, working, wh...\n",
      "1      business  [nestle, to, suspend, many, products, in, russ...\n",
      "2      politics  [capitol, rioters, called, nancy, pelosi, s, o...\n",
      "3      business  [johnson, johnson, plans, to, split, into, two...\n",
      "4        travel                              [my, trip, to, egypt]\n",
      "...         ...                                                ...\n",
      "5936   business  [peloton, insiders, sold, nearly, million, in,...\n",
      "5937     travel                    [cotswolds, quite, picturesque]\n",
      "5938     travel  [just, came, back, from, a, trip, to, amsterda...\n",
      "5939  parenting  [is, it, ok, to, check, the, baby, monitor, at...\n",
      "5940   business  [self, driving, waymo, trucks, to, haul, loads...\n",
      "\n",
      "[5941 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "def preprocessing(data):\n",
    "    #make all characters to lower case\n",
    "    data['text']=data['text'].apply(lambda x: str.lower(x))\n",
    "\n",
    "    #word tokenization\n",
    "    data['text'] = data['text'].apply(lambda x: wordpunct_tokenize(x))\n",
    "\n",
    "    #delete stopwords and punctuation\n",
    "    stopwordList = set(stopwords.words('english') + list(string.punctuation))\n",
    "    data['text'] = data['text'].apply(lambda x: list(word for word in x if word.isalpha()))\n",
    "\n",
    "    print(data)\n",
    "    return data\n",
    "\n",
    "dataset = preprocessing(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of word dict:  17677\n",
      "0       [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, ...\n",
      "1       [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "2       [0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, ...\n",
      "3       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "4       [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "                              ...                        \n",
      "5936    [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "5937    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "5938    [1, 2, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, ...\n",
      "5939    [15, 8, 14, 10, 11, 7, 5, 5, 4, 0, 3, 6, 11, 4...\n",
      "5940    [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: bagOfWords, Length: 5941, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "def makingBagOfWordsCol(data):\n",
    "    allTextList = sum(data['text'].tolist(),[])\n",
    "    numOfdict = len(set(allTextList))\n",
    "    print('the number of word dict: ',numOfdict)\n",
    "    \n",
    "    #the number of words to use for training\n",
    "    numOfFeatureWords = int(numOfdict*0.95)\n",
    "\n",
    "    fdist = FreqDist(allTextList)\n",
    "\n",
    "    wordDict = list(word for word, freq in fdist.most_common(numOfFeatureWords))\n",
    "\n",
    "    def bagOfWords(tokens):\n",
    "        d = defaultdict(int,{ word:0 for word in wordDict })\n",
    "\n",
    "        for token in tokens:\n",
    "            d[token]+=1\n",
    "\n",
    "        ret = []\n",
    "\n",
    "        for key, val in d.items():\n",
    "            ret.append(val)\n",
    "\n",
    "        return ret[:numOfFeatureWords]\n",
    "\n",
    "    data['bagOfWords'] = data['text'].apply(lambda x: bagOfWords(x))\n",
    "    return data\n",
    "\n",
    "dataset = makingBagOfWordsCol(dataset)\n",
    "\n",
    "print(dataset['bagOfWords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "divide trainset and testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainSize:  4158\n",
      "testSize:  1782\n",
      "4158\n",
      "1783\n"
     ]
    }
   ],
   "source": [
    "trainSplit = 0.7\n",
    "testSplit = 0.3\n",
    "\n",
    "datasetSize = dataset.shape[0]\n",
    "\n",
    "trainSize = int(datasetSize * trainSplit)\n",
    "testSize = int(datasetSize * testSplit)\n",
    "\n",
    "print('trainSize: ', trainSize)\n",
    "print('testSize: ', testSize)\n",
    "\n",
    "trainSet = dataset.iloc[:trainSize, :]\n",
    "testSet = dataset.iloc[trainSize:, :]\n",
    "\n",
    "print(trainSet.shape[0])\n",
    "print(testSet.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a Bag of Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4158, 16793) (4158,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "classifier = MultinomialNB()\n",
    "trainX = np.array(trainSet['bagOfWords'].tolist())\n",
    "trainY = np.array(trainSet['category'].tolist())\n",
    "print(trainX.shape, trainY.shape)\n",
    "classifier.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1783, 16793) (1783,)\n",
      "accuracy : 0.8536174985978687\n",
      "[[2.30842083e-032 1.29014272e-030 9.99999970e-001 5.93177885e-033\n",
      "  2.81372117e-040 3.01653441e-008]\n",
      " [9.66655711e-001 2.38529777e-002 9.49058345e-007 3.21792043e-003\n",
      "  6.02899141e-003 2.43450098e-004]\n",
      " [1.47046059e-006 3.72800759e-003 1.12150299e-008 9.96270068e-001\n",
      "  3.95971623e-007 4.63220606e-008]\n",
      " ...\n",
      " [4.06442601e-012 1.23649827e-010 9.17573238e-001 6.66788001e-012\n",
      "  1.09827772e-012 8.24267617e-002]\n",
      " [3.04510346e-300 2.41853358e-259 1.00000000e+000 1.04319799e-262\n",
      "  9.97469132e-320 2.12884942e-192]\n",
      " [8.81282946e-001 1.68604498e-002 3.19169407e-005 2.95384142e-003\n",
      "  1.12718492e-002 8.75989969e-002]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "testX = np.array(testSet['bagOfWords'].tolist())\n",
    "testY = np.array(testSet['category'].tolist())\n",
    "print(testX.shape, testY.shape)\n",
    "predY = classifier.predict(testX)\n",
    "predYProb = classifier.predict_proba(testX)\n",
    "accuracy = accuracy_score(testY, predY)\n",
    "print('accuracy :', accuracy)\n",
    "print(predYProb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read news category dataset and concat headline and short_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Who Would Win in a Fight Between Iron Man and ...\n",
      "1        Here's The 2015 March Madness Schedule For Sat...\n",
      "2        14 Wineries Where You Can Stay the Night (PHOT...\n",
      "3        Mike Pence To America: Trump Never Said Those ...\n",
      "4        Flight Attendant’s Hilarious In-Flight Safety ...\n",
      "                               ...                        \n",
      "78177    Getting Our Pennies in Priority for Tax Day We...\n",
      "78178    Travel Ban Is A Minor Win For Trump And A Majo...\n",
      "78179    Iceland: 3 Mistakes Many Travelers Make After ...\n",
      "78180    Republicans Hold On To Arizona House Seat That...\n",
      "78181    Top GOP Senator Challenges Trump Arms Deals Ov...\n",
      "Name: text, Length: 78182, dtype: object\n"
     ]
    }
   ],
   "source": [
    "originNewsCategory = ['BUSINESS', 'ENTERTAINMENT', 'PARENTING', 'POLITICS', 'SPORTS', 'TRAVEL']\n",
    "\n",
    "newsDataset = pd.read_json(\"./dataset/News_category_Dataset_v2.json\", lines=True)\n",
    "newsDataset = newsDataset[newsDataset['category'].isin(originNewsCategory)]\n",
    "newsDataset['category'] = newsDataset['category'].apply(lambda x: x.lower())\n",
    "\n",
    "newsDataset['text'] = newsDataset['headline'] + ' ' + newsDataset['short_description']\n",
    "newsDataset = newsDataset[['text','category']]\n",
    "#shuffle row\n",
    "newsDataset=newsDataset.sample(frac=1).reset_index(drop=True)\n",
    "print(newsDataset['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprossesing and make bag of words columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsDataset = preprocessing(newsDataset)\n",
    "\n",
    "newsDataset = makingBagOfWordsCol(newsDataset)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9920a9191bbd0883129cd4e7291d7025cf6acd0cb1f54d8654d4c35c2558a4dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
